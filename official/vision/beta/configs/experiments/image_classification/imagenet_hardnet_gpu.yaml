runtime:
  distribution_strategy: 'mirrored'
  # mixed_precision_dtype: 'float16'
  # loss_scale: 'dynamic'

  num_gpus: 1

task:
  model:
    num_classes: 1001
    input_size: [256, 256, 3]
    backbone:
      type: 'hardnet'
      hardnet:
        model_id: 70
    dropout_rate: 0.2
    norm_activation:
      activation: 'relu'
      norm_epsilon: 0.001
      norm_momentum: 0.99
      use_sync_bn: true
  losses:
    l2_weight_decay: 0.00001
    one_hot: true
    label_smoothing: 0.1
  train_data:
    input_path: ''
    tfds_data_dir: '/home/whizz/data'
    tfds_name: 'imagenet2012'
    tfds_split: 'train'
    sharding: true
    is_training: true

    global_batch_size: 256  # 128 * 8

    dtype: 'float16'
    aug_rand_hflip: true
    aug_policy: randaug
    randaug_magnitude: 5 # 5-10 for image classification
  validation_data:
    input_path: ''
    tfds_data_dir: '/home/whizz/data'
    tfds_name: 'imagenet2012'
    tfds_split: 'validation'
    sharding: true
    is_training: true
    global_batch_size: 256  # 128 * 8
    dtype: 'float16'
    drop_remainder: false
trainer:

  train_steps: 2502000  # 500 epochs
  validation_steps: 1281167
  validation_interval: 5004
  steps_per_loop: 5004  # NUM_EXAMPLES (1281167) // global_batch_size
  summary_interval: 80064
  checkpoint_interval: 80064

  optimizer_config:
    learning_rate:
      type: 'exponential'
      exponential:

        initial_learning_rate: 0.001  # 0.008 * batch_size / 128
        decay_steps: 15012  # 2.5 * steps_per_epoch
        decay_rate: 0.96

        staircase: true
    warmup:
      type: 'linear'
      linear:

        warmup_steps: 25020
