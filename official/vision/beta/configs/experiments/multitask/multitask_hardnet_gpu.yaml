# See https://arxiv.org/pdf/1706.05587.pdf ablation study
# See https://arxiv.org/pdf/1802.02611.pdf for training configurations
runtime:
  distribution_strategy: 'mirrored'
  # mixed_precision_dtype: 'float32'  

  num_gpus: 1 ###

task:
  model:
    input_size: [256, 256, 3]  # has to match task_config's
    backbone:
      type: 'hardnet'
      hardnet:
        model_id: 70
    norm_activation:
      activation: 'relu'
      norm_epsilon: 0.001
      norm_momentum: 0.9997 # 0.99 for segmentation
      use_sync_bn: true
    l2_weight_decay: 0.0001
    heads: [
      {
        name: "classification", # has to match task_configs's
        init_checkpoint_modules: 'backbone',
        num_classes: 4, # has to match task_configs's
        head: {
          level: 0,
          num_convs: 2,
          num_filters: 256,
          add_head_batch_norm: false,
          dropout_rate: 0.2}
      },
      {
        name: "segmentation", # has to match task_configs's
        init_checkpoint_modules: 'all',
        num_classes: 19, # has to match task_configs's
        decoder: {
          type: 'hardnet',
          hardnet: {
            model_id: 70}},
        head: {
          level: 0,
          num_convs: 0,
          feature_fusion: null,
          low_level: 0,
          low_level_num_filters: 0}
      },
    ]
  init_checkpoint: '/home/whizz/experiments/imagenetbase/deeplabv3plus_hardnet_scooter_gpu_256x256_imagenetbase2_notranslate/ckpt-864000'
  task_routines: !!python/tuple
  - task_name: segmentation  # has to match model's
    task_weight: 1.0
    eval_steps: null
    task_config:
      model:
        input_size: [256, 256, 3]  # has to match model's
        num_classes: 19  # has to match model's
      losses:
        class_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
        ignore_label: 250
        top_k_percent_pixels: 0.3
      train_data:
        output_size: [256, 256]
        input_path: '/mnt/ssd2/tfrecords/seg_train*'

        global_batch_size: 16 ###

        is_training: true
        aug_rand_hflip: true
        aug_scale_min: 0.5
        aug_scale_max: 2.0
        preserve_aspect_ratio: false
        aug_policy: randaug
        randaug_magnitude: 5
        randaug_available_ops: [AutoContrast, Equalize, Invert, Rotate, Posterize,
          Solarize, Color, Contrast, Brightness, Sharpness, Cutout, SolarizeAdd]
      validation_data:
        output_size: [256, 256]
        input_path: '/mnt/ssd2/tfrecords/seg_val*'
        global_batch_size: 1
        is_training: false
        preserve_aspect_ratio: false
        resize_eval_groundtruth: true
        drop_remainder: false
  - task_name: classification  # has to match model's
    task_weight: 1.0
    eval_steps: null
    task_config:
      model:
        input_size: [256, 256, 3]  # has to match model's
        num_classes: 4  # has to match model's
      losses:
        label_smoothing: 0.1
        one_hot: false
      train_data:
        input_path: '/mnt/ssd2/tfrecords/cls_env_train*'

        global_batch_size: 16 ###

        is_training: true
        aug_rand_hflip: true
        aug_scale_min: 0.5
        aug_scale_max: 2.0
        preserve_aspect_ratio: false
        aug_policy: randaug
        randaug_magnitude: 5
      validation_data:
        input_path: '/mnt/ssd2/tfrecords/cls_env_val*'
        global_batch_size: 1
        is_training: false
        preserve_aspect_ratio: false
        drop_remainder: false
trainer:
  trainer_type: joint # interleaving no train_loop_end for logging yet
  task_sampler:
    proportional:
      alpha: 1.0
    type: proportional

  steps_per_loop: 432         # train_steps_per_epoch
  summary_interval: 6915      # 2 * train_steps_per_epoch
  checkpoint_interval: 6915   # 2 * train_steps_per_epoch
  train_steps: 864000         # epochs * steps_per_epoch
  validation_steps: 6915      # val_steps_per_epoch
  validation_interval: 432    # train_steps_per_epoch
  
  best_checkpoint_eval_metric: 'mean_iou' ###
  continuous_eval_timeout: 3600
  max_to_keep: 100
  optimizer_config:
    ema: null
    learning_rate:
      polynomial:
        cycle: false

        decay_steps: 864000   # 2.5-5 * train_steps_per_epoch (train_steps for poly)
        end_learning_rate: 0.0
        initial_learning_rate: 0.002

        name: PolynomialDecay
        power: 0.9
      type: polynomial
    optimizer:
      sgd:
        clipnorm: null
        clipvalue: null
        decay: 0.0
        global_clipnorm: null
        momentum: 0.9
        name: SGD
        nesterov: false
      type: sgd
    warmup:
      linear:
        name: linear
        warmup_learning_rate: 0

        warmup_steps: 2160    # 5 * steps_per_epoch

      type: linear
