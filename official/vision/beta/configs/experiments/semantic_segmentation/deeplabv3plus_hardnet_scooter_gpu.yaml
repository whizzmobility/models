# See https://arxiv.org/pdf/1706.05587.pdf ablation study
# See https://arxiv.org/pdf/1802.02611.pdf for training configurations
runtime:
  distribution_strategy: 'mirrored'
  # mixed_precision_dtype: 'float32'
  num_gpus: 3
task:
  model:
    num_classes: 19
    input_size: [256, 256, 3]
    backbone:
      type: 'hardnet'
      hardnet:
        model_id: 70
    decoder:
      type: 'hardnet'
      hardnet:
        model_id: 70
    head:
      level: 0
      num_convs: 0
      feature_fusion: null
      low_level: 0
    norm_activation:
      activation: 'relu'
      norm_epsilon: 0.001
      norm_momentum: 0.99
      use_sync_bn: true
  losses:
    l2_weight_decay: 0.0001 ###
    ignore_label: 250 ###
    top_k_percent_pixels: 0.3  # only backpropagate loss for the topk 100% pixels.
    class_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
  train_data:
    output_size: [256, 256] ### [512, 1024]
    input_path: 'D:/data/test_data/val**'
    is_training: true
    global_batch_size: 12 ### 16
    dtype: 'float32'
    aug_rand_hflip: true
    aug_scale_max: 2.0
    aug_scale_min: 0.5
    preserve_aspect_ratio: false
    bright_min: 1.0
    bright_max: 1.0
    rotate_min: 0.0 # in radians
    rotate_max: 0.0
    aug_policy: randaug
    randaug_magnitude: 5 # 5-10 for image classification
    randaug_available_ops: ['AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize', 'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness', 'Cutout', 'SolarizeAdd']
  validation_data:
    output_size: [256, 256] ### [1024, 2048]
    input_path: 'D:/data/test_data/val**'
    is_training: false
    global_batch_size: 1 ### 16
    dtype: 'float32'
    preserve_aspect_ratio: false
    drop_remainder: false
    resize_eval_groundtruth: true
  init_checkpoint: '/home/whizz/experiments/imagenet/imagenet_hardnet_gpu_256x256_randaug/ckpt-2502000'
  init_checkpoint_modules: 'backbone'
trainer:
  optimizer_config:
    learning_rate:
      polynomial:
        decay_steps: 345000 # train step_per_epoch * 200
        initial_learning_rate: 0.007
        end_learning_rate: 0.0 ###
        power: 0.9
      type: polynomial
    optimizer:
      sgd:
        momentum: 0.9
      type: sgd
    warmup:
      linear:
        name: linear
        warmup_learning_rate: 0
        warmup_steps: 3450 # train steps_per_epoch * 2
      type: linear
  steps_per_loop: 1725      # train steps_per_epoch
  summary_interval: 1725    # train steps_per_epoch
  train_steps: 345000      # train steps_per_epoch * 200
  validation_interval: 1725 # train steps_per_epoch
  validation_steps: 6915    # val steps_per_epoch
  checkpoint_interval: 1725 # train steps_per_epoch
  best_checkpoint_eval_metric: 'mean_iou' ###
  continuous_eval_timeout: 3600
