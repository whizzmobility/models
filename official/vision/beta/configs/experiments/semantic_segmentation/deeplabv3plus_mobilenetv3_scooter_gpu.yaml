# See https://arxiv.org/pdf/1706.05587.pdf ablation study
# See https://arxiv.org/pdf/1802.02611.pdf for training configurations
runtime:
  distribution_strategy: 'mirrored'
  # mixed_precision_dtype: 'float32'
  num_gpus: 3
task:
  model:
    num_classes: 19
    input_size: [256, 256, 3]
    backbone:
      type: mobilenet
      mobilenet:
        model_id: MobileNetV3Large
        stochastic_depth_drop_rate: 0.2
        filter_size_scale: 1.0 # % of filters, 0 to 1
        output_stride: 16
    decoder:
      aspp:
        level: 17 # ln(output_stride)/ln(2)
        dilation_rates: [6, 12, 18] ###
        ### pool_kernel_size: [512, 1024]
    head:
      level: 17
      num_convs: 2
      feature_fusion: 'deeplabv3plus'
      low_level: 2
      low_level_num_filters: 48
    norm_activation:
      activation: 'swish'
      norm_epsilon: 0.00001
      norm_momentum: 0.99
      use_sync_bn: true
  losses:
    l2_weight_decay: 0.0001 ###
    ignore_label: 250 ###
    ### top_k_percent_pixels: 1.0  # only backpropagate loss for the topk 100% pixels.
  train_data:
    output_size: [256, 256] ### [512, 1024]
    train_on_crops: true
    input_path: 'D:/data/test_data/val**'
    is_training: true
    global_batch_size: 16
    dtype: 'float32'
    aug_rand_hflip: true
    aug_scale_max: 2.0
    aug_scale_min: 0.5
  validation_data:
    output_size: [256, 256] ### [1024, 2048]
    input_path: 'D:/data/test_data/val**'
    is_training: false
    global_batch_size: 1 ### 16
    dtype: 'float32'
    drop_remainder: false
    resize_eval_groundtruth: true
# See https://arxiv.org/pdf/1807.11626.pdf
trainer:
  optimizer_config:
    learning_rate:
      exponential:
        initial_learning_rate: 0.001
        decay_steps: 2160 # train steps per epoch * 5
        decay_rate: 0.97
      type: exponential
    optimizer:
      rmsprop:
        rho: 0.9
        momentum: 0.9
        epsilon: 0.002
      type: rmsprop
    warmup:
      linear:
        name: linear
        warmup_learning_rate: 0
        warmup_steps: 432 # train steps_per_epoch
      type: linear
  steps_per_loop: 432      # train steps_per_epoch
  summary_interval: 432    # train steps_per_epoch
  train_steps: 216000      # train steps_per_epoch * 500
  validation_interval: 432 # train steps_per_epoch
  validation_steps: 6915    # val steps_per_epoch
  checkpoint_interval: 432 # train steps_per_epoch
  best_checkpoint_eval_metric: 'mean_iou' ###
  continuous_eval_timeout: 3600
