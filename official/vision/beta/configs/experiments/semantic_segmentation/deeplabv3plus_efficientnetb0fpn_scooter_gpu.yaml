# See https://arxiv.org/pdf/1706.05587.pdf ablation study
# See https://arxiv.org/pdf/1802.02611.pdf for training configurations
runtime:
  distribution_strategy: 'mirrored'
  # mixed_precision_dtype: 'float32'
  
  num_gpus: 2

task:
  model:
    num_classes: 19
    input_size: [256, 256, 3]
    backbone:
      type: 'efficientnet'
      efficientnet:
        model_id: b0
        # output_stride: 16
        # stem_type: 'v1'
        se_ratio: 0.0
        stochastic_depth_drop_rate: 0.2
        # multigrid: [1, 2, 4]
    decoder:
      fpn:
        num_filters: 256
        use_separable_conv: False
      type: 'fpn'
    head:
      level: 2 # upscales to same as min level
      num_convs: 3
      feature_fusion: 'pyramid_fusion'
    norm_activation:
      activation: 'relu'
      norm_epsilon: 0.001
      norm_momentum: 0.99
      use_sync_bn: true
  losses:
    l2_weight_decay: 0.0001 ###
    ignore_label: 250 ###
    top_k_percent_pixels: 0.3  # only backpropagate loss for the topk 100% pixels.
    class_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
  train_data:
    output_size: [256, 256]
    input_path: ' /mnt/ssd2/tfrecords/seg_train**'
    is_training: true
    
    global_batch_size: 16
    
    dtype: 'float32'
    aug_rand_hflip: true
    aug_scale_max: 2.0
    aug_scale_min: 0.5
    preserve_aspect_ratio: false
    bright_min: 1.0
    bright_max: 1.0
    rotate_min: 0.0 # in radians
    rotate_max: 0.0
    aug_policy: randaug
    randaug_magnitude: 5 # 5-10 for image classification
    randaug_available_ops: ['AutoContrast', 'Equalize', 'Invert', 'Rotate', 'Posterize', 'Solarize', 'Color', 'Contrast', 'Brightness', 'Sharpness', 'Cutout', 'SolarizeAdd']
  validation_data:
    output_size: [256, 256]
    input_path: '/mnt/ssd2/tfrecords/seg_val**'
    is_training: false
    global_batch_size: 1 ### 16
    dtype: 'float32'
    preserve_aspect_ratio: false
    drop_remainder: false
    resize_eval_groundtruth: true
  init_checkpoint: ''
  init_checkpoint_modules: 'backbone'
trainer:
  optimizer_config:
    learning_rate:
      exponential:
        
        initial_learning_rate: 0.016
        decay_steps: 4320   # 2.5-5 * train_steps_per_epoch (train_steps for poly)
        
        decay_rate: 0.97
      type: exponential
    optimizer:
      rmsprop:
        rho: 0.9
        momentum: 0.9
        epsilon: 0.002
      type: rmsprop
    warmup:
      linear:
        name: linear
        warmup_learning_rate: 0
        
        warmup_steps: 4320  # 5 * train_steps_per_epoch
      
      type: linear
  
  steps_per_loop: 864       # train_steps_per_epoch
  summary_interval: 1728    # 2 * train_steps_per_epoch
  train_steps: 864000       # epochs * train steps_per_epoch
  validation_interval: 864  # train_steps_per_epoch
  validation_steps: 6915    # val_steps_per_epoch
  checkpoint_interval: 864  # 2 * train_steps_per_epoch
  
  best_checkpoint_eval_metric: 'mean_iou' ###
  continuous_eval_timeout: 3600
